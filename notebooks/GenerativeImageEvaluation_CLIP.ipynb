{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G9yiLls99AZV"
   },
   "outputs": [],
   "source": [
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"openai/clip-vit-base-patch32\"  # lightweight, fast\n",
    "model = CLIPModel.from_pretrained(model_name).to(device)\n",
    "processor = CLIPProcessor.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FHL0Ro2A9LjG",
    "outputId": "b048f59a-b3ef-4264-b6d9-bc3491c817b6"
   },
   "outputs": [],
   "source": [
    "\n",
    "image_urls = [\n",
    "  # AI-generated images (get from free tools)\n",
    "  # \"https://api.dicebear.com/7.x/avataaars/svg?seed=FireflyGenAI\", # Removed SVG URL\n",
    "\n",
    "  # Real images (e.g., from Unsplash)\n",
    "  \"https://images.unsplash.com/photo-1501594907352-04cda38ebc29\",\n",
    "  \"https://images.unsplash.com/photo-1529626455594-4ff0802cfb7e\"\n",
    "]\n",
    "\n",
    "def load_image_from_url(url):\n",
    "    resp = requests.get(url)\n",
    "    return Image.open(BytesIO(resp.content)).convert(\"RGB\")\n",
    "\n",
    "image_paths = []\n",
    "images = []\n",
    "for i, url in enumerate(image_urls):\n",
    "    img = load_image_from_url(url)\n",
    "    images.append(img)\n",
    "    fname = f\"img_{i}.jpg\"\n",
    "    img.save(fname)\n",
    "    image_paths.append(fname)\n",
    "\n",
    "image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tmg2PBc39PRo"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Example prompts describing image intent (or the prompt used to generate them)\n",
    "prompts = [\n",
    "    \"A cityscape at sunset\",\n",
    "    \"A realistic portrait of a woman\",\n",
    "\n",
    "\n",
    "]\n",
    "\n",
    "# If using image URLs without known prompts, use generic prompts for alignment checks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uN_oCH1H9Rag"
   },
   "outputs": [],
   "source": [
    "\n",
    "def embed_image(image_path_or_pil):\n",
    "    if isinstance(image_path_or_pil, str):\n",
    "        image = Image.open(image_path_or_pil).convert(\"RGB\")\n",
    "    else:\n",
    "        image = image_path_or_pil\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        img_embeds = model.get_image_features(**inputs)\n",
    "        img_embeds = img_embeds / img_embeds.norm(p=2, dim=-1, keepdim=True)\n",
    "    return img_embeds.cpu().numpy()\n",
    "\n",
    "def embed_text(texts):\n",
    "    inputs = processor(text=texts, return_tensors=\"pt\", padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        txt_embeds = model.get_text_features(**inputs)\n",
    "        txt_embeds = txt_embeds / txt_embeds.norm(p=2, dim=-1, keepdim=True)\n",
    "    return txt_embeds.cpu().numpy()\n",
    "\n",
    "# compute embeddings for images\n",
    "image_embeddings = []\n",
    "for p in image_paths:\n",
    "    image_embeddings.append(embed_image(p))\n",
    "\n",
    "text_embeddings = embed_text(prompts)  # shape: (n_prompts, dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "id": "vnZuEsuz9Un2",
    "outputId": "275c589d-bee3-4b84-ba7b-ed57910fb797"
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# If #images == #prompts and you want pairwise alignment:\n",
    "scores = []\n",
    "for i, img_emb in enumerate(image_embeddings):\n",
    "    # compare to the corresponding prompt (if aligned 1:1)\n",
    "    if i < len(prompts):\n",
    "        sim = cosine_similarity(img_emb, text_embeddings[i:i+1])[0,0]\n",
    "    else:\n",
    "        # fallback: compare to first prompt or mean of prompts\n",
    "        sim = cosine_similarity(img_emb, text_embeddings.mean(axis=0, keepdims=True))[0,0]\n",
    "    scores.append(float(sim))\n",
    "\n",
    "# present scores\n",
    "df = pd.DataFrame({\"image\": image_paths, \"alignment_score\": scores})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "id": "IyfpciHU9Xhg",
    "outputId": "7fd79a0f-8ca1-4b59-c393-5c6ec39d74d5"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Provide or upload/reference a few high-quality reference images (HQ)\n",
    "# e.g., place HQ images in ./hq_images/\n",
    "hq_folder = \"hq_images\"\n",
    "hq_paths = []  # fill with paths to your HQ images\n",
    "# Example: if you uploaded or saved hq images, populate hq_paths accordingly\n",
    "\n",
    "# compute HQ embeddings\n",
    "hq_embeddings = [embed_image(p) for p in hq_paths] if hq_paths else []\n",
    "\n",
    "def aesthetic_score(img_emb, hq_embs):\n",
    "    if len(hq_embs)==0:\n",
    "        # fallback: use overall brightness/entropy as a primitive aesthetic proxy\n",
    "        return None\n",
    "    sims = [cosine_similarity(img_emb, he)[0,0] for he in hq_embs]\n",
    "    return float(np.mean(sims))\n",
    "\n",
    "aesthetic_scores = []\n",
    "for emb in image_embeddings:\n",
    "    sc = aesthetic_score(emb, hq_embeddings)\n",
    "    aesthetic_scores.append(sc)\n",
    "\n",
    "df[\"aesthetic_score\"] = aesthetic_scores\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "id": "Eq-N6McB9agR",
    "outputId": "9871930b-7a8c-4eaf-89ad-eeb5de7afd9e"
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.barh(df[\"image\"], df[\"alignment_score\"])\n",
    "plt.xlabel(\"Alignment (cosine similarity)\")\n",
    "plt.title(\"Image-Text Alignment Scores\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def evaluate_image(img, prompt=\"A high-quality photograph\"):\n",
    "    # img is PIL Image\n",
    "    img_emb = embed_image(img)\n",
    "    txt_emb = embed_text([prompt])\n",
    "    align = float(cosine_similarity(img_emb, txt_emb)[0,0])\n",
    "    # aesthetic (if HQ embeddings exist)\n",
    "    aest = aesthetic_score(img_emb, hq_embeddings) if hq_embeddings else None\n",
    "    return {\"alignment_score\": round(align,4), \"aesthetic_score\": round(aest,4) if aest else \"N/A\"}\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=evaluate_image,\n",
    "    inputs=[gr.Image(type=\"pil\"), gr.Textbox(lines=1, placeholder=\"Describe expected content or style\")],\n",
    "    outputs=[\"label\", \"label\"],\n",
    "    title=\"Generative Image Evaluator (CLIP)\",\n",
    "    description=\"Upload an image and a prompt â€” returns alignment and (optional) aesthetic score.\"\n",
    ")\n",
    "demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (.venv) gen-eval",
   "language": "python",
   "name": "gen-eval-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
